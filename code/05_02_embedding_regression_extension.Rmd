---
title: "Embedding regression"
author: "Jae Yeon Kim"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load packages 

```{r}

# Import pkgs
pacman::p_load(
  tidyverse,
  lubridate, 
  purrr,
  furrr,
  tm,
  quanteda, 
  here,
  tidytext, # text analysis 
  textclean,
  textstem, 
  ggthemes,
  text2vec, # word embedding 
  widyr,
  patchwork, # arranging ggplots
  glue,
  # deep learning 
  text2vec,
  # network graph
  igraph,
  ggraph,
  # structural breakpoint
  strucchange,
  pbapply,
  kableExtra
)

devtools::install_github("prodriguezsosa/conText")

library(conText)

theme_set(theme_clean())

source(here("functions", "embedding.r"))
```


## Load data 

```{r}
df <- read.csv(here("processed_data/cleaned_text.csv")) 

df <- df %>%
  mutate(group = if_else(str_detect(source, "Arab"), "Arab", "Indian"))

df <- df %>%
  select(c("text","source","date", "group"))

filipino <- read.csv(here("processed_data", "filipino.csv")) %>%
  select(c("text","source","date")) %>%
  mutate(group = "Filipino")

aapi <- read.csv(here("processed_data", "aapi.csv")) %>%
  select(c("text","source","date")) %>%
  mutate(group = "AAPI")

aapi$date <- dmy(aapi$date)
filipino$date <- dmy(filipino$date)

df$date <- as.Date(df$date)
```

```{r}
corpus <- bind_rows(df, filipino, aapi)
```

```{r eval = FALSE}
unique(corpus$group)

unique(corpus$source)
```

```{r}
# Create dummy variables 
corpus$intervention <- if_else(corpus$date >= as.Date("2001-09-11"), 1, 0)

write.csv(corpus, here("processed_data", "corpus.csv"))

corpus <- read.csv(here("processed_data", "corpus.csv"))
```

## Preproceessing text 

```{r}
corpus$clean_text <- clean_text(corpus$text)

corpus <- corpus %>% 
  filter(!str_detect(clean_text, "film|cartoon|festival|music|dance"))

corpus_copy <- corpus
```

```{r}
corpus_copy %>%
  group_by(group) %>%
  summarise(n = n(),
            min = min(date),
            max = max(date))

nrow(corpus_copy)

corpus_copy %>%
  group_by(source) %>%
  summarise(n = n())
```

```{r eval = FALSE}
corpus$terror <- str_detect(corpus$clean_text, "terror")

corpus %>%
  group_by(group, intervention) %>%
  summarize(n = sum(terror))

(83 - 5)/5
(371 - 85)/85
(265 - 55)/55
(853 - 304)/304
```

group
<chr>
intervention
<int>
n
<int>
AAPI	0	5		
AAPI	1	83		
Arab	0	85		
Arab	1	371		
Filipino	0	55		
Filipino	1	265		
Indian	0	304		
Indian	1	853	

# Arab + Indian  

## Subsetted data 

```{r}
corpus <- corpus_copy %>%
  filter(group %in% c("Arab", "Indian")) %>%
  mutate(group = if_else(str_detect(group, "Arab"), 1, 0))
```

## Local embedding and matrix 

```{r}
set.seed(20201008L)
local_glove <- df2vec(corpus)
local_transform <- df2ltm(corpus, local_glove)

save(local_glove, local_transform, 
     file = here("processed_data/context_bg_ai.Rdata"))
```

## ALC embedding approach 


```{r}
load(file = here("processed_data/context_bg_ai.Rdata"))

quant_corpus <- quanteda::corpus(
  corpus, 
  text_field = "clean_text")

# remove unncessary elements 
toks <- quanteda::tokens(quant_corpus,  
                         remove_punct = T, 
                         remove_symbols = T, 
                         remove_numbers = T, 
                         remove_separators = T)

# only use features that appear at least 5 times in the corpus
feats <- dfm(toks, verbose = TRUE) %>% 
  dfm_trim(min_termfreq = 5) %>% 
  featnames()

# leave the pads 
toks <- tokens_select(toks, feats, padding = TRUE)

mod1 <- conText(formula = terror ~ pattern.intervention + pattern.group, 
          data = toks, 
          pre_trained = local_glove,
          transform = TRUE, 
          transform_matrix = local_transform, 
          bootstrap = TRUE, 
          num_bootstraps = 20, 
          stratify = TRUE,
          permute = TRUE, 
          num_permutations = 200, 
          window = 6, 
          valuetype = 'fixed', 
          case_insensitive = TRUE, 
          hard_cut = FALSE,
          verbose = FALSE)

plot_tibble1 <- mod1@normed_cofficients %>% 
  as_tibble() %>%
  mutate(coefficient = c("9/11 attacks", "Arab American")) %>% 
  mutate(coefficient = factor(coefficient, levels = coefficient))

plot_tibble1
```

```{r}
reg_plot1 <- plot_tibble1 %>%
  ggplot(aes(x = coefficient, y = normed.estimate)) + 
  geom_bar(position = position_dodge(), 
    stat = "identity", width = 0.5) + 
  geom_errorbar(aes(ymin = normed.estimate - 
    1.96 * std.error, ymax = normed.estimate + 1.96 * std.error), size = 0.75, width = 0.15, 
    position = position_dodge(0.9)) + 
  ylab("Norm of beta hats") + 
  theme(axis.text.x = element_text(size = 10, angle = 90, vjust = 0.5, hjust = 1),
          axis.text.y = element_text(size = 10)) +
  labs(
    title = "Arab and Indian Amerian newspaper comparison",
    subtitle = "Keyword: Terror")

reg_plot1

ggsave(here("output", "embedreg_ai.png"))
```

## Nearest neighbors 

```{r}
set.seed(20201008L)
arab_post_bt <- get_bt_terms(1, 1, "terror", 30, "Arab Americans", "Indian Americans")
indian_post_bt <- get_bt_terms(1, 0, "terror", 30, "Arab Americans", "Indian Americans")

arab_pre_bt <- get_bt_terms(0, 1, "terror", 30, "Arab Americans", "Indian Americans")
indian_pre_bt <- get_bt_terms(0, 0, "terror", 30, "Arab Americans", "Indian Americans")
```

```{r}
terms2plot_sep(arab_pre_bt, indian_pre_bt, "terror", "Pre-9/11") + terms2plot_sep(arab_post_bt, indian_post_bt, "terror", "Post-9/11") +
  plot_annotation(tag_levels = 'A') & 
  theme(plot.tag = element_text(size = 8))

ggsave(here("output", "btterrorsep_ai.png"), width = 12, height = 10)
```

# Arab + Fipilino

## Subsetted data 

```{r}
corpus <- corpus_copy %>%
  filter(group %in% c("Arab", "Filipino")) %>%
  mutate(group = if_else(str_detect(group, "Arab"), 1, 0))
```

## Local embedding and matrix 

```{r}
set.seed(20201008L)
local_glove <- df2vec(corpus)
local_transform <- df2ltm(corpus, local_glove)

save(local_glove, local_transform, 
     file = here("processed_data/context_bg_af.Rdata"))
```

## ALC embedding approach 


```{r}
load(file = here("processed_data/context_bg_af.Rdata"))

quant_corpus <- quanteda::corpus(
  corpus, 
  text_field = "clean_text")

# remove unncessary elements 
toks <- quanteda::tokens(quant_corpus,  
                         remove_punct = T, 
                         remove_symbols = T, 
                         remove_numbers = T, 
                         remove_separators = T)

# only use features that appear at least 5 times in the corpus
feats <- dfm(toks, verbose = TRUE) %>% 
  dfm_trim(min_termfreq = 5) %>% 
  featnames()

# leave the pads 
toks <- tokens_select(toks, feats, padding = TRUE)
     
mod2 <- conText(formula = terror ~ pattern.intervention + pattern.group, 
          data = toks, 
          pre_trained = local_glove,
          transform = TRUE, 
          transform_matrix = local_transform, 
          bootstrap = TRUE, 
          num_bootstraps = 20, 
          stratify = TRUE,
          permute = TRUE, 
          num_permutations = 200, 
          window = 6, 
          valuetype = 'fixed', 
          case_insensitive = TRUE, 
          hard_cut = FALSE,
          verbose = FALSE)

plot_tibble2 <- mod2@normed_cofficients %>% 
  as_tibble() %>%
  mutate(coefficient = c("9/11 attacks", "Arab American")) %>% 
  mutate(coefficient = factor(coefficient, levels = coefficient))

plot_tibble2
```

```{r}
reg_plot2 <- plot_tibble2 %>%
  ggplot(aes(x = coefficient, y = normed.estimate)) + 
  geom_bar(position = position_dodge(), 
    stat = "identity", width = 0.5) + 
  geom_errorbar(aes(ymin = normed.estimate - 
    1.96 * std.error, ymax = normed.estimate + 1.96 * std.error), size = 0.75, width = 0.15, 
    position = position_dodge(0.9)) + 
  ylab("Norm of beta hats") + 
  theme(axis.text.x = element_text(size = 10, angle = 90, vjust = 0.5, hjust = 1),
          axis.text.y = element_text(size = 10)) +
  labs(title = "Arab and Filipino American newspaper comparison",
       subtitle = "Keyword: Terror")

reg_plot2

ggsave(here("output", "embedreg_af.png"))
```

## Nearest neighbors 

```{r}
set.seed(20201008L)
arab_post_bt <- get_bt_terms(1, 1, "terror", 30, "Arab Americans", "Filipino Americans")
filipino_post_bt <- get_bt_terms(1, 0, "terror", 30, "Arab Americans", "Filipino Americans")

arab_pre_bt <- get_bt_terms(0, 1, "terror", 30, "Arab Americans", "Filipino Americans")
filipino_pre_bt <- get_bt_terms(0, 0, "terror", 30, "Arab Americans", "Filipino Americans")
```

```{r}
terms2plot_sep(arab_pre_bt, filipino_pre_bt, "terror", "Pre-9/11") + terms2plot_sep(arab_post_bt, filipino_post_bt, "terror", "Post-9/11") + 
  plot_annotation(tag_levels = 'A') & 
  theme(plot.tag = element_text(size = 8))

ggsave(here("output", "btterrorsep_af.png"), width = 12, height = 10)
```

# Arab + AAPI

## Subsetted data 

```{r}
corpus <- corpus_copy %>%
  filter(group %in% c("Arab", "AAPI")) %>%
  mutate(group = if_else(str_detect(group, "Arab"), 1, 0))
```

## Local embedding and matrix 

```{r}
set.seed(20201008L)
local_glove <- df2vec(corpus)
local_transform <- df2ltm(corpus, local_glove)

save(local_glove, local_transform, 
     file = here("processed_data/context_bg_aa.Rdata"))
```

## ALC embedding approach 

```{r}
load(here("processed_data/context_bg_aa.Rdata"))

quant_corpus <- quanteda::corpus(
  corpus, 
  text_field = "clean_text")

# remove unncessary elements 
toks <- quanteda::tokens(quant_corpus,  
                         remove_punct = T, 
                         remove_symbols = T, 
                         remove_numbers = T, 
                         remove_separators = T)

# only use features that appear at least 5 times in the corpus
feats <- dfm(toks, verbose = TRUE) %>% 
  dfm_trim(min_termfreq = 5) %>% 
  featnames()

# leave the pads 
toks <- tokens_select(toks, feats, padding = TRUE)
     
mod3 <- conText(formula = terror ~ pattern.intervention + pattern.group, 
          data = toks, 
          pre_trained = local_glove,
          transform = TRUE, 
          transform_matrix = local_transform, 
          bootstrap = TRUE, 
          num_bootstraps = 20, 
          stratify = TRUE,
          permute = TRUE, 
          num_permutations = 200, 
          window = 6, 
          valuetype = 'fixed', 
          case_insensitive = TRUE, 
          hard_cut = FALSE,
          verbose = FALSE)

plot_tibble3 <- mod3@normed_cofficients %>% 
  as_tibble() %>%
  mutate(coefficient = c("9/11 attacks", "Arab American")) %>% 
  mutate(coefficient = factor(coefficient, levels = coefficient))

plot_tibble3
```


```{r}
reg_plot3 <- plot_tibble3 %>%
  ggplot(aes(x = coefficient, y = normed.estimate)) + 
  geom_bar(position = position_dodge(), 
    stat = "identity", width = 0.5) + 
  geom_errorbar(aes(ymin = normed.estimate - 
    1.96 * std.error, ymax = normed.estimate + 1.96 * std.error), size = 0.75, width = 0.15, 
    position = position_dodge(0.9)) + 
  ylab("Norm of beta hats") + 
  theme(axis.text.x = element_text(size = 10, angle = 90, vjust = 0.5, hjust = 1),
          axis.text.y = element_text(size = 10)) +
  labs(title = "Arab and Asian American newspaper comparison",
       subtitle = "Keyword: Terror")

reg_plot3

ggsave(here("output", "embedreg_aa.png"))
```

## Nearest neighbors 

```{r}
set.seed(20201008L)
arab_post_bt <- get_bt_terms(1, 1, "terror", 30, "Arab Americans", "Asian Americans")
aa_post_bt <- get_bt_terms(1, 0, "terror", 30, "Arab Americans", "Asian Americans")

arab_pre_bt <- get_bt_terms(0, 1, "terror", 30, "Arab Americans", "Asian Americans")
aa_pre_bt <- NULL
```

```{r}
terms2plot_sep(arab_pre_bt, aa_pre_bt, "terror", "Pre-9/11") + terms2plot_sep(arab_post_bt, aa_post_bt, "terror", "Post-9/11") +
  plot_annotation(tag_levels = 'A') & 
  theme(plot.tag = element_text(size = 8))

ggsave(here("output", "btterrorsep_aa.png"), width = 12, height = 10)
```

# Summary 

## Plot 

```{r}
sums <- data.frame(estimate = 
             c(mod1@normed_cofficients$normed.estimate[1],
             mod2@normed_cofficients$normed.estimate[1],
             mod3@normed_cofficients$normed.estimate[1], mod1@normed_cofficients$normed.estimate[2],
             mod2@normed_cofficients$normed.estimate[2],
             mod3@normed_cofficients$normed.estimate[2]),
           std.error = 
             c(mod1@normed_cofficients$std.error[1], mod2@normed_cofficients$std.error[1], mod3@normed_cofficients$std.error[1],
               mod1@normed_cofficients$std.error[2], mod2@normed_cofficients$std.error[2], mod3@normed_cofficients$std.error[2]),
           group = rep(c("Indian American", "Filipino American", "Asian American"),2),
           type = rep(c("Intervention", "Group"), each = 3))
```

```{r}
sums %>%
  ggplot(aes(x = fct_reorder(group, estimate), y = estimate, ymax = estimate + 1.96*std.error, ymin = estimate - 1.96*std.error, col = type)) +
  geom_pointrange() +
  labs(title = "Overall context comparison",
       x = "Comparison group",
       y = "Norm of beta hats",
       col = "Covavriate") +
  geom_text(label = round(sums$estimate, 2),
            position = position_dodge(width = 0.9),
            vjust = -0.5) +
  scale_fill_viridis_d()

ggsave(here("output", "embedreg_all.png"))
```

# Reading documents 

```{r}
corpus$terror <- str_detect(corpus$clean_text, "terror")

corpus %>%
  filter(intervention == 0) %>%
  group_by(group) %>%
  summarize(avg = sum(terror))
```

```{r}
corpus %>%
  # Post 9/11
  filter(intervention == 1) %>%
  # Arab newspaper 
  filter(group == 1) %>%
  # Search documents including the term "immigrant"
  filter(str_detect(text, "immigrant|Immigrant|immigrants|Immigrants")) %>%
  # Select the 3rd row (the 3rd document)
  slice(3) %>%
  # Pull the date and newspaper
  pull(date, source)
```

