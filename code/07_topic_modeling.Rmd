---
title: "Topic modeling analysis"
author: "Jae Yeon Kim"
output:
html_document:
  toc: true
  theme: united
---

# Setup

## 0. Setup

I tweaked the global option of the R Markdown to enlarge figures produced by ggplot2.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8,
                      echo = FALSE, warning = FALSE, message = FALSE) # global setting for enlarging image size
```

```{r}

# Clean up the environment

rm(list = ls())

# Import libraries (adapted from this link: https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # for the tidyverse framework
        irr, # for calculating inter-coder reliability score
        corrplot, # for visualizing correlation coefficients
        ggpubr, # for arranging ggplots
        ggthemes, # for fancy ggplot themes
        tidytext, # for tidytext
        stm, # for structural topic modeling
        tidystm, # for extracting estimated effects from stm
        furrr, # for multiprocessing
        patchwork # for arranging images
)

```

## 1. Import files

```{r}

full_articles <- read_csv( "/home/jae/ITS-Text-Classification/processed_data/df.csv")
```

## 2. Clean and wrangle data

```{r}

# Manipulate strings
full_articles$source <- str_trim(gsub(".*:", "", full_articles$source))

# Dropping the first column 
full_articles <- full_articles[, -1]

```

## 3. Structural topic modeling

I adapted the code from Julia Silge's [amazing STM tutotial](https://juliasilge.com/blog/evaluating-stm/).

### 3.1. Preprocessing

```{r}

# Clean text function

clean_text <- function(data){

  data <- data %>%
    mutate(text = tolower(text),
    text = str_replace_all(text, '[\n]',''),
    text = removePunctuation(text),
    text = qdap::bracketX(text),
    text = trimws(text),
    postID = row_number())

  return(data)
}

# Apply to each data

full_articles <- clean_text(full_articles)

# Test 
full_articles$text[1]

## Select the articles classified either domestic or nondomestic 
do_articles <- subset(full_articles, domestic == 1)
nondo_articles <- subset(full_articles, domestic == 0)

```

```{r}
# Create a document-term matrix

do_articles_sparse <- do_articles %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  filter(!str_detect(word, "[0-9]+")) %>%
  add_count(word) %>%
  filter(n > 100) %>%
  select(-n) %>%
  count(postID, word) %>%
  cast_sparse(postID, word, n)

nondo_articles_sparse <- nondo_articles %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  filter(!str_detect(word, "[0-9]+")) %>%
  add_count(word) %>%
  filter(n > 100) %>%
  select(-n) %>%
  count(postID, word) %>%
  cast_sparse(postID, word, n)

```

### 3.2. Train and evaluate a topic model

#### 3.2.1. Train many models

```{r}

plan(multiprocess)

do_many_models <- tibble(K = c(5, 10, 15)) %>%
               mutate(topic_model = future_map(K, ~stm(full_articles_sparse, 
                                                       K = .,
                                                       verbose = TRUE)))

nondo_many_models <- tibble(K = c(5, 10, 15)) %>%
               mutate(topic_model = future_map(K, ~stm(nondo_articles_sparse, 
                                                       K = .,
                                                       verbose = TRUE)))

```

#### 3.2.2. Visualize diagnostics

```{r}

visualize_diagnostics <- function(sparse_matrix, many_models){

heldout <- make.heldout(sparse_matrix)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, sparse_matrix),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, sparse_matrix),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")}

```

```{r}

# Set theme to theme base
theme_set(theme_base())

do_diag <- visualize_diagnostics(do_articles_sparse, do_many_models)

nondo_diag <- visualize_diagnostics(nondo_articles_sparse, nondo_many_models) 

do_diag / nondo_diag

ggsave("/home/jae/ITS-Text-Classification/outputs/topic_modeling_diagnosis.png", height = 10)

```


### 3.3. Adding covariates

```{r}

do_stm <- stm(documents = do_articles_sparse,
              data = do_articles,
              K = 10,
              prevalence =~ intervention + group,
              verbose = TRUE)

nondo_stm <- stm(documents = nondo_articles_sparse,
              data = nondo_articles,
              K = 10,
              prevalence =~ intervention + group,
              verbose = TRUE)
  
# Save these heavy files
write_rds(do_stm, "/home/jae/ITS-Text-Classification/processed_data/do_stm.rds")
write_rds(nondo_stm, "/home/jae/ITS-Text-Classification/processed_data/nondo_stm.rds")

```

### 3.4. Estimating effects

```{r}

# Prep
do_predict_topics <- estimateEffect(formula = 1:10 ~ intervention + group, stmobj = do_stm, metadata = do_articles, uncertainty = "Global")

nondo_predict_topics <- estimateEffect(formula = 1:10 ~ intervention + group, stmobj = nondo_stm, metadata = nondo_articles, uncertainty = "Global")

```

```{r}
# Plotting estimated effects
do_predict_group <- extract.estimateEffect(do_predict_topics, "group",
                       model = stm,
                       method = "pointestimate") %>%
  ggplot(aes(x = fct_reorder(factor(as.character(topic)), estimate), y = estimate, ymax = ci.upper, ymin = ci.lower, col = covariate.value)) +
  geom_pointrange() + theme_base() +
  labs(x = "Topics",
       y = "Estimated effects",
       col = "Group",
       title = "Structural topic modeling results") +
       theme(legend.position = 'bottom') +
       ylim(c(0, 0.4))

nondo_predict_group <- extract.estimateEffect(nondo_predict_topics, "group",
                       model = stm,
                       method = "pointestimate") %>%
  ggplot(aes(x = fct_reorder(factor(as.character(topic)), estimate), y = estimate, ymax = ci.upper, ymin = ci.lower, col = covariate.value)) +
  geom_pointrange() + theme_base() +
  labs(x = "Topics",
       y = "Estimated effects",
       col = "Group",
       title = "Structural topic modeling results") +
       theme(legend.position = 'bottom') +
       ylim(c(0, 0.4))

ggsave("/home/jae/ITS-Text-Classification/outputs/topic_predict_estimates.png", width = 12)

```
