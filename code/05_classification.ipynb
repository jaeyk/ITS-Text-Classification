{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification \n",
    "\n",
    "This part was mostly done by my wonderful RAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include relevant imports here\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in scraped articles from csv file to dataframe\n",
    "articles = pd.read_csv('/home/jae/ITS-Text-Classification/raw_data/sample_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>group</th>\n",
       "      <th>date</th>\n",
       "      <th>intervention</th>\n",
       "      <th>expanding</th>\n",
       "      <th>distancing</th>\n",
       "      <th>assimilating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Third Rate Or Not, No Meddling Please! By UPEN...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1997-10-24</td>\n",
       "      <td>post</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phagwah Parade Draws Many By DHARMVIR GEHLAUT ...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2001-03-23</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advani Blames Congress And UF DALTONGUNJ, Biha...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1998-01-23</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violence Feared During Assembly Elections By N...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indrani Rahman's Final Bow By ARUN A. AGUIAR P...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1999-02-19</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>President Bush offers Ramadan greeting Preside...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-12-15</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>US officials vow to fight bigotry \"We are list...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-11-05</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>Al-Jazeera Bureau Chief Hafez Mirazi will be t...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2002-02-15</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>There are literally thousands of Arab American...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2002-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Protests at Mosque Waving Confederate flags in...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-11-05</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Third Rate Or Not, No Meddling Please! By UPEN...   \n",
       "1     Phagwah Parade Draws Many By DHARMVIR GEHLAUT ...   \n",
       "2     Advani Blames Congress And UF DALTONGUNJ, Biha...   \n",
       "3     Violence Feared During Assembly Elections By N...   \n",
       "4     Indrani Rahman's Final Bow By ARUN A. AGUIAR P...   \n",
       "...                                                 ...   \n",
       "1010  President Bush offers Ramadan greeting Preside...   \n",
       "1011  US officials vow to fight bigotry \"We are list...   \n",
       "1012  Al-Jazeera Bureau Chief Hafez Mirazi will be t...   \n",
       "1013  There are literally thousands of Arab American...   \n",
       "1014  Protests at Mosque Waving Confederate flags in...   \n",
       "\n",
       "                      source   group       date intervention  expanding  \\\n",
       "0         News India - Times  Indian 1997-10-24         post        0.0   \n",
       "1         News India - Times  Indian 2001-03-23         post        NaN   \n",
       "2         News India - Times  Indian 1998-01-23         post        NaN   \n",
       "3         News India - Times  Indian 2001-01-05         post        NaN   \n",
       "4         News India - Times  Indian 1999-02-19         post        NaN   \n",
       "...                      ...     ...        ...          ...        ...   \n",
       "1010  The Arab American View    Arab 2001-12-15          pre        1.0   \n",
       "1011  The Arab American View    Arab 2001-11-05          pre        1.0   \n",
       "1012  The Arab American View    Arab 2002-02-15          pre        1.0   \n",
       "1013  The Arab American View    Arab 2002-04-28          pre        1.0   \n",
       "1014  The Arab American View    Arab 2001-11-05          pre        1.0   \n",
       "\n",
       "      distancing  assimilating  \n",
       "0            1.0           0.0  \n",
       "1            NaN           NaN  \n",
       "2            NaN           NaN  \n",
       "3            NaN           NaN  \n",
       "4            NaN           NaN  \n",
       "...          ...           ...  \n",
       "1010         0.0           0.0  \n",
       "1011         0.0           0.0  \n",
       "1012         0.0           0.0  \n",
       "1013         0.0           1.0  \n",
       "1014         0.0           0.0  \n",
       "\n",
       "[1015 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select relevant columns and convert date column into Datetime format\n",
    "articles = articles[['text', 'source', 'group', 'date', 'intervention', 'expanding', 'distancing', 'assimilating']]\n",
    "articles['date'] =  pd.to_datetime(articles['date'], format='%Y%m%d')\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>group</th>\n",
       "      <th>date</th>\n",
       "      <th>intervention</th>\n",
       "      <th>expanding</th>\n",
       "      <th>distancing</th>\n",
       "      <th>assimilating</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Third Rate Or Not, No Meddling Please! By UPEN...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1997-10-24</td>\n",
       "      <td>post</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phagwah Parade Draws Many By DHARMVIR GEHLAUT ...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2001-03-23</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advani Blames Congress And UF DALTONGUNJ, Biha...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1998-01-23</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violence Feared During Assembly Elections By N...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indrani Rahman's Final Bow By ARUN A. AGUIAR P...</td>\n",
       "      <td>News India - Times</td>\n",
       "      <td>Indian</td>\n",
       "      <td>1999-02-19</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>President Bush offers Ramadan greeting Preside...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-12-15</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>US officials vow to fight bigotry \"We are list...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-11-05</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>Al-Jazeera Bureau Chief Hafez Mirazi will be t...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2002-02-15</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>There are literally thousands of Arab American...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2002-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Protests at Mosque Waving Confederate flags in...</td>\n",
       "      <td>The Arab American View</td>\n",
       "      <td>Arab</td>\n",
       "      <td>2001-11-05</td>\n",
       "      <td>pre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Third Rate Or Not, No Meddling Please! By UPEN...   \n",
       "1     Phagwah Parade Draws Many By DHARMVIR GEHLAUT ...   \n",
       "2     Advani Blames Congress And UF DALTONGUNJ, Biha...   \n",
       "3     Violence Feared During Assembly Elections By N...   \n",
       "4     Indrani Rahman's Final Bow By ARUN A. AGUIAR P...   \n",
       "...                                                 ...   \n",
       "1010  President Bush offers Ramadan greeting Preside...   \n",
       "1011  US officials vow to fight bigotry \"We are list...   \n",
       "1012  Al-Jazeera Bureau Chief Hafez Mirazi will be t...   \n",
       "1013  There are literally thousands of Arab American...   \n",
       "1014  Protests at Mosque Waving Confederate flags in...   \n",
       "\n",
       "                      source   group       date intervention  expanding  \\\n",
       "0         News India - Times  Indian 1997-10-24         post        0.0   \n",
       "1         News India - Times  Indian 2001-03-23         post        NaN   \n",
       "2         News India - Times  Indian 1998-01-23         post        NaN   \n",
       "3         News India - Times  Indian 2001-01-05         post        NaN   \n",
       "4         News India - Times  Indian 1999-02-19         post        NaN   \n",
       "...                      ...     ...        ...          ...        ...   \n",
       "1010  The Arab American View    Arab 2001-12-15          pre        1.0   \n",
       "1011  The Arab American View    Arab 2001-11-05          pre        1.0   \n",
       "1012  The Arab American View    Arab 2002-02-15          pre        1.0   \n",
       "1013  The Arab American View    Arab 2002-04-28          pre        1.0   \n",
       "1014  The Arab American View    Arab 2001-11-05          pre        1.0   \n",
       "\n",
       "      distancing  assimilating  category  \n",
       "0            1.0           0.0         1  \n",
       "1            NaN           NaN         0  \n",
       "2            NaN           NaN         0  \n",
       "3            NaN           NaN         0  \n",
       "4            NaN           NaN         0  \n",
       "...          ...           ...       ...  \n",
       "1010         0.0           0.0         1  \n",
       "1011         0.0           0.0         1  \n",
       "1012         0.0           0.0         1  \n",
       "1013         0.0           1.0         1  \n",
       "1014         0.0           0.0         1  \n",
       "\n",
       "[1015 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding column to determine if there is relevant info (expanding, distancing, assimilating) or not \n",
    "list = []\n",
    "for x in articles['assimilating'].isnull().values:\n",
    "    if x == True:\n",
    "        list.append(0)\n",
    "    else:\n",
    "        list.append(1)\n",
    "\n",
    "# Create new binary column called category based on this\n",
    "# 1 indicates domestic issue, 0 indicates non-domestic issue\n",
    "articles['category'] = list\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    574\n",
       "0    441\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the balance of target variables \n",
    "\n",
    "articles['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Conduct train-test split where 90% of data used for training\n",
    "train, val = train_test_split(articles, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/jae/anaconda3/lib/python3.6/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: six in /home/jae/.local/lib/python3.6/site-packages (from nltk) (1.13.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the article data by removing html, punctuation, new-line breaks (\\n), and stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "!pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Converting all data into lowercase and defining functions to remove extraneous elemets\n",
    "train['text'] = train['text'].str.lower()\n",
    "val['text'] = val['text'].str.lower()\n",
    "compiling = re.compile(r'<[^>]+>')\n",
    "                       \n",
    "def remove_html(x):\n",
    "    return compiling.sub('', x)\n",
    "def remove_punctuation(x):\n",
    "    return x.translate(str.maketrans('','', string.punctuation))\n",
    "def remove_n(x):\n",
    "    return x.replace('\\n', '')\n",
    "removal_words = stopwords.words('english')\n",
    "\n",
    "# Applying functions to clean train \n",
    "train['text'] = train.agg({\"text\": [remove_html]})\n",
    "train['text'] = train.agg({'text': [remove_punctuation]})\n",
    "train['text'] = train.agg({'text': [remove_n]})\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join([y for y in x.split() if y not in removal_words]))\n",
    "# Applying functions to clean test\n",
    "val['text'] = val.agg({\"text\": [remove_html]})\n",
    "val['text'] = val.agg({'text': [remove_punctuation]})\n",
    "val['text'] = val.agg({'text': [remove_n]})\n",
    "val['text'] = val['text'].apply(lambda x: \" \".join([y for y in x.split() if y not in removal_words ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NA</th>\n",
       "      <th>Not NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>put</th>\n",
       "      <td>113.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intellectual</th>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perversity</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>431.0</td>\n",
       "      <td>493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whenever</th>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kalifatul</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twostorey</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spires</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onestorey</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complimented</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38342 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NA  Not NA\n",
       "put           113.0   126.0\n",
       "intellectual   11.0    20.0\n",
       "perversity      1.0     0.0\n",
       "like          431.0   493.0\n",
       "whenever       12.0    17.0\n",
       "...             ...     ...\n",
       "kalifatul       0.0     1.0\n",
       "twostorey       0.0     1.0\n",
       "spires          0.0     2.0\n",
       "onestorey       0.0     1.0\n",
       "complimented    0.0     1.0\n",
       "\n",
       "[38342 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count all of the words that are in NA vs not in NA\n",
    "from collections import Counter\n",
    "NA_count = Counter(\" \".join(train[train['category'] == 0]['text']).split())\n",
    "not_NA_count = Counter(\" \".join(train[train['category'] == 1]['text']).split())\n",
    "\n",
    "# Creating dataframe named testing to hold these counts\n",
    "testing = pd.DataFrame({\"NA\": NA_count, \"Not NA\": not_NA_count}).fillna(0)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['party', 'bjp', 'congress', 'minister', 'film', 'gandhi',\n",
       "       'israeli', 'chief', 'says', 'films', 'sharif', 'leader', 'seats',\n",
       "       'election', 'government', 'temple', 'army', 'delhi', 'prime',\n",
       "       'former', 'janata', 'parties', 'may', 'alliance', 'jerusalem',\n",
       "       'set', 'soldiers', 'palestinian', 'palestinians', 'front', 'power',\n",
       "       'khan', 'old', 'nuclear', 'land', 'vajpayee', 'home', 'site',\n",
       "       'left', 'five', 'hindu', 'forces', 'indias', 'form', 'jewish',\n",
       "       'elections', 'months', 'west', 'gujarat', 'away', 'coalition',\n",
       "       'majority', 'mother', 'asia', 'situation', 'governments',\n",
       "       'opposition', 'christians', 'mosque', 'yet', 'man', 'four',\n",
       "       'though', 'music', 'making', 'next', 'lebanon', 'back', 'better',\n",
       "       'lot', 'little', 'visit', 'court', 'despite', 'far', '10',\n",
       "       'movement', 'came', 'killed', 'military', 'went', 'give',\n",
       "       'conference', 'within', 'known', 'british', 'past', 'took',\n",
       "       'family', 'christian', 'still', 'however', 'society', 'much',\n",
       "       'ago', 'earlier', 'future', 'later', 'days', 'great', 'put',\n",
       "       'three', 'second', 'major', 'million', 'among', 'long', 'case',\n",
       "       'children', 'good', 'first', 'social', 'take', 'given', 'singh',\n",
       "       'done', 'times', 'role', 'two', 'others', 'men', 'day', 'go',\n",
       "       'never', 'without', 'hindus', 'recent', 'violence', 'year',\n",
       "       'program', 'even', 'asked', 'used', 'organization', 'police',\n",
       "       'women', 'since', 'become', 'economic', 'get', 'every', 'always',\n",
       "       'going', 'week', 'end', 'years', 'history', 'well', 'held',\n",
       "       'another', 'political', 'place', 'several', 'right', 'foreign',\n",
       "       'would', 'life', 'last', 'according', 'percent', 'issue', 'number',\n",
       "       'kashmir', 'around', 'fact', 'work', 'different', 'dont', 'say',\n",
       "       'could', 'city', 'think', 'time', 'see', 'local', 'international',\n",
       "       'general', 'media', 'council', 'dr', 'want', 'like', 'way',\n",
       "       'leaders', 'important', 'added', 'made', 'know', 'israel', 'help',\n",
       "       'article', 'director', 'group', 'islamic', 'part', 'called',\n",
       "       'come', 'security', 'need', 'war', 'human', 'peace', 'countries',\n",
       "       'including', 'officials', 'south', 'news', 'school', 'policy',\n",
       "       'law', 'university', 'support', 'east', 'pakistan', 'national',\n",
       "       'york', 'president', 'make', 'state', 'asian', 'house',\n",
       "       'religious', 'center', 'must', 'issues', 'told', 'religion',\n",
       "       'public', 'islam', 'groups', 'report', 'members', 'terrorism',\n",
       "       'india', 'also', 'america', 'one', 'new', 'many', 'united',\n",
       "       'rights', 'country', 'indian', 'states', 'world', 'people',\n",
       "       'muslims', 'muslim', 'community', 'us', 'arab', 'said', 'american'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make column of times NA appeared more than not NA\n",
    "testing['NA distinct'] = testing['NA'] - testing['Not NA']\n",
    "testing = testing.sort_values(by = 'NA distinct', ascending = False)\n",
    "\n",
    "# Make column of percentage of words\n",
    "testing['NA percent'] = testing['NA'] / (testing['NA'] +  testing['Not NA'])\n",
    "tester = testing[testing['Not NA'] > 0].sort_values(by = 'NA percent', ascending = False)\n",
    "\n",
    "# Above work leads to creating of the top 100 words that appear in NA articles, as seen in words areray\n",
    "words = tester[tester['NA'] > 100].sort_values(by = 'NA distinct', ascending = False).head(300).reset_index()['index'].values.astype(str)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f72adf59d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m ohc_words_x_train = pd.SparseDataFrame(cv.fit_transform(train['text']), \n\u001b[1;32m      6\u001b[0m                        \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        cv.get_feature_names())\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mohc_words_x_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object() takes no parameters"
     ]
    }
   ],
   "source": [
    "# Create counts of how often the number of times top 100 words appeared in all articles, with 1 and 2 grams\n",
    "cv = CountVectorizer(vocabulary=words, ngram_range = (1, 2))\n",
    "\n",
    "# Encode the count vectorizer to create a dataframe holding counts\n",
    "ohc_words_x_train = pd.SparseDataFrame(cv.fit_transform(train['text']), \n",
    "                       train.index,\n",
    "                       cv.get_feature_names())\n",
    "ohc_words_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get addition features from one hot encoding the source, intervention, and group columns\n",
    "features_x_train = pd.concat([pd.get_dummies(train[col]) for col in ['source', 'intervention', 'group']], axis=1)\n",
    "features_x_train = features_x_train.drop(columns = [\"The Arab American View\"])\n",
    "features_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final training matrix from the above two dataframes created from cells\n",
    "xx_train = pd.merge(left=ohc_words_x_train, left_index=True\n",
    "                  ,right=features_x_train, right_index=True,\n",
    "                  how='inner')\n",
    "xx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit our Logistic Regression model with L1 regularization and determine the training accuracy\n",
    "yy_train = train['category']\n",
    "NA_model = LogisticRegressionCV(fit_intercept = True, penalty = 'l1', solver = 'saga')\n",
    "NA_model.fit(xx_train, yy_train)\n",
    "\n",
    "accuracy = NA_model.score(xx_train, yy_train)\n",
    "print(\"Training Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for word count matrix in testing\n",
    "cv = CountVectorizer(vocabulary=words, ngram_range = (1, 2))\n",
    "\n",
    "ohc_words_x_test = pd.SparseDataFrame(cv.fit_transform(val['text']), \n",
    "                       val.index,\n",
    "                       cv.get_feature_names(), \n",
    "                       default_fill_value=0)\n",
    "ohc_words_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for feature matrix in testing \n",
    "features_x_test = pd.concat([pd.get_dummies(val[col]) for col in ['source', 'intervention', 'group']], axis=1)\n",
    "features_x_test = features_x_test.drop(columns = [\"The Arab American View\"])\n",
    "features_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging above dataframes to get the final testing matrix \n",
    "xx_test = pd.merge(left=ohc_words_x_test, left_index=True\n",
    "                  ,right=features_x_test, right_index=True,\n",
    "                  how='inner')\n",
    "xx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain test accuracy score from trained model\n",
    "accuracy = NA_model.score(xx_test, val['category'])\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unlabeled data and follow procedures in the beginning of notebook to clean\n",
    "unlabeled_articles = pd.read_csv('unlabeled_articles.csv', index_col=0)\n",
    "unlabeled_articles['date'] =  pd.to_datetime(unlabeled_articles['date'], format='%Y%m%d')\n",
    "unlabeled_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize whether article pertains to Arab-American or Indian-American issues based on publication source\n",
    "list = []\n",
    "for x in unlabeled_articles['source']:\n",
    "    if x == \"The Arab American News\":\n",
    "        list.append(\"Arab\")\n",
    "    else:\n",
    "        list.append(\"Indian\")\n",
    "\n",
    "unlabeled_articles['group'] = list\n",
    "unlabeled_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count encoding dataframe of top words again, except for unlabeled data\n",
    "cv = CountVectorizer(vocabulary=words, ngram_range = (1, 2))\n",
    "\n",
    "ohc_words_x_unlabeled = pd.SparseDataFrame(cv.fit_transform(unlabeled_articles['text']), \n",
    "                       unlabeled_articles.index,\n",
    "                       cv.get_feature_names(), \n",
    "                       default_fill_value=0)\n",
    "ohc_words_x_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, except with the source, intervetion, and group features\n",
    "features_x_unlabeled = pd.get_dummies(unlabeled_articles, columns=['source', 'intervention', 'group'])\n",
    "features_x_unlabeled = features_x_unlabeled.drop(columns = ['text', 'date'])\n",
    "features_x_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data matrix created here\n",
    "xx_unlabeled = pd.merge(left=ohc_words_x_unlabeled, left_index=True\n",
    "                  ,right=features_x_unlabeled, right_index=True,\n",
    "                  how='inner')\n",
    "xx_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our model to predict whether the articles pertained to domestic vs non-domestic isues\n",
    "values = NA_model.predict(xx_unlabeled)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our results into a csv file\n",
    "pd.DataFrame(values, xx_unlabeled.index).to_csv(\"/home/jae/ITS-Text-Classification/processed_data/final_values_predicted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
