---
title: "NLP"
author: "Jae Yeon Kim"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load packages 

```{r}
# Import pkgs
pacman::p_load(
  tidyverse,
  lubridate, 
  purrr,
  furrr,
  tm,
  quanteda, 
  here,
  tidytext, # text analysis 
  textclean,
  textstem, 
  text2vec, 
  spacyr, 
  ggthemes,
  # deep learning 
  kableExtra
)

theme_set(theme_clean())

source(here("functions", "embedding.r"))
source(here("functions", "utils.r"))
```


## Load data 

```{r}
corpus <- read.csv(here("processed_data", "eda_version.csv"))

dim(corpus)
```

```{r}
parse_corpus <- spacy_parse(corpus$text, tag = FALSE, lemma = FALSE, entity = TRUE, nounphrase = TRUE)

geo_list <- entity_extract(parse_corpus, type = "named") %>%
  filter(entity_type %in% c("GPE", "LOC"))

org_list <- entity_extract(parse_corpus, type = "named") %>%
  filter(entity_type == "ORG")

person_list <- entity_extract(parse_corpus, type = "named") %>%
  filter(entity_type == "PERSON")

save(geo_list, org_list, person_list, file = here("processed_data", "entity_list.RData"))
```

```{r}
load(here("processed_data", "entity_list.RData"))
```

# Unique lists 

```{r}
corpus$doc_id <- glue("text{1:nrow(corpus)}")

geo_df <- left_join(geo_list, corpus %>%
                        select(text, date, source, group, intervention, doc_id))

saveRDS(geo_df, here("processed_data", "geo_df.rds"))

label_geo <- data.frame(entity = unique(geo_df$entity))

saveRDS(label_geo, here("processed_data", "label_geo.rds"))
```

```{r}
us_geo <- add_US_location(df = label_geo)

geo_joined <- left_join(geo_df, us_geo)

geo_tagged <- geo_joined %>%
  group_by(doc_id, date, intervention, group) %>%
  summarize(mean = mean(US_location)) %>%
  mutate(us_dummy = if_else(mean > 0, 1, 0))
```

```{r}
all <- geo_tagged %>%
  group_by(intervention, group) %>%
  summarise(mean = mean(us_dummy),
            ci.high = ci.high(us_dummy),
            ci.low = ci.low(us_dummy)) %>%
  mutate(Duration = "10 Year")

one_year <- geo_tagged %>%
  mutate(date = as.Date(date)) %>%
  filter(date >= as.Date("2000-01-1") & date <= as.Date("2002-12-31")) %>%
  group_by(intervention, group) %>%
  summarise(mean = mean(us_dummy),
            ci.high = ci.high(us_dummy),
            ci.low = ci.low(us_dummy)) %>%
  mutate(Duration = "2 Year")

combined <- bind_rows(all, one_year)
```

```{r}
combined %>%
  mutate(intervention = recode(intervention, 
    `0` = "Pre-9/11",
    `1` = "Post-9/11")) %>%
  mutate(intervention = factor(intervention, levels = c("Pre-9/11", "Post-9/11"))) %>%
  ggplot(aes(x = factor(intervention), y = mean, ymax = mean + ci.high, ymin = mean - ci.low, col = Duration)) +
  geom_pointrange() +
  facet_wrap(~group) +
  ggrepel::geom_text_repel(aes(label = scales::percent(round(mean, 2)))) +
  labs(x = "", y = "% of articles mentioned US states, cities, and counties",
       col = "Window size",
       title = "Increasing interest in US",
       subtitle = "All articles mentioned Muslim/Muslims") +
  scale_y_continuous(labels = scales::percent)

ggsave(here("output", "mean_diff.png"))
```

```{r}
geo_joined %>%
  filter(group == "Arab" & intervention == 0) %>%
  filter(US_location == 1) %>%
  select(text) %>%
  unique()
```