---
title: "Additional text analysis"
author: "Jae Yeon Kim"
output:
html_document:
  toc: True
  theme: united
  number_sections: True
---

# Setup

```{r}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # the tidyverse framework
        ggpubr, # arranging ggplots
        ggthemes, # fancy ggplot themes
        tidytext, # tidytext
        patchwork, # arranging images,
        purrr, # functional programming
        here, # reproducibility
        nytimes, # NYT analysis 
        jsonlite, # parsing JSON
        glue # gluing objects and strings 
)


# devtools::install_github("jaeyk/makereproducible")
library(makereproducible)

# Import R scripts

script_list <- list.files(paste0(here::here(), "/functions"),
  pattern = "*.r|*.R",
  full.names = TRUE
)

for (i in 1:length(script_list))
{
  source(script_list[[i]])
}
```

# NYT data analysis 

```{r}
Sys.setenv(nyt_key = "<insert key>")
key <- Sys.getenv("nyt_key")
```

```{r}
# Parameters 

begin_date <- "19960911"

end_date <- "20060911"

term <- "muslim+muslims"

baseurl <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q="

# URL request 

url_request <- glue("{baseurl}{term}&begin_date={begin_date}&end_date={end_date}&facet_filter=true&api-key={key}")

# Extract function 

extract_nyt_data <- function(i){
  
  # JSON object 
  out <- fromJSON(glue("{url_request}&page={i}"), flatten = TRUE) %>% 
    data.frame() 
  
  # Select fields
  out <- out[,c("response.docs.news_desk", "response.docs.section_name", "response.docs.subsection_name", "response.docs.type_of_material", "response.docs._id", "response.docs.headline.main")]
  
  # Page numbering 
  out$page <- i
  
  message(glue("Scraping {i} page"))
  
  return(out)
  
}

# Making the function to process slow 

# 6 seconds sleep is the default requirement
slowly_extract <- slowly(extract_nyt_data,
                         rate = rate_delay(pause = 7,
           max_times = 200))
```


```{r}
# Extract function 

extract_all <- function(page_list) {

  df <- map(page_list, safely(slowly_extract)) 
  
  compact_df <- df %>% 
    map_dfr("result") %>%
    compact() 
  
  return(compact_df)
  
}
```

```{r}
# Looping the function over the list

max_pages <- round((fromJSON(url_request)$response$meta$hits[1] / 10) - 1)

interval <- function(x) {
  
  out <- c(x:(x + 200))

  return(out)
  
}

# I created this list of numeric vectors to avoid API rate limit. 

vec_list <- map(seq(0, max_pages, by = 200), interval)
```


```{r}
combined_df <- map(vec_list, extract_all)
```

```{r}
saveRDS(combined_df, file = here("processed_data/nyt_articles.Rdata"))
```